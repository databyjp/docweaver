{
    "objective": "Create comprehensive documentation for the new SPFresh vector index type",
    "context": "## SPFresh Index Type - Technical Description\n\n**Overview:**\nSPFresh is a hybrid disk-based vector index that supports incremental in-place updates for billion-scale datasets. It maintains partition centroids in an in-memory HNSW index for fast candidate selection, while storing vector partitions on disk. Unlike traditional approaches that require periodic global rebuilds, SPFresh uses LIRE (Lightweight Incremental REbalancing) to maintain index quality through local partition splits and vector reassignments.\n\n**Architecture:**\n- **In-memory HNSW index**: Stores partition centroids for fast nearest-partition lookup\n- **Disk-based partitions**: Store the actual vectors, organized by proximity\n- **Background rebalancing**: Automatically maintains partition quality as data changes\n\n**How Search Works:**\n1. Query vector searches the in-memory HNSW index to find nearest partition centroids\n2. Identified partitions are loaded from disk in parallel\n3. Full distance calculations performed on candidate vectors\n4. Top-K results returned\n\n**How Updates Work:**\n1. Insert: Vector assigned to nearest partition (via HNSW centroid search), appended to disk\n2. When partition exceeds `maxPostingSize`, it splits into two partitions\n3. New centroids are computed and updated in the HNSW index\n4. Nearby partitions checked for vectors needing reassignment (NPA compliance)\n5. Only boundary vectors are reassigned, minimizing overhead\n\n**Key Features:**\n- **Fast partition selection**: HNSW index on centroids provides O(log n) partition lookup\n- **In-place updates**: Append vectors directly without requiring global index rebuilds\n- **Incremental rebalancing**: Maintains index quality through local splits and reassignments\n- **Low resource overhead**: Requires ~1% of DRAM and <10% of CPU cores compared to global rebuild\n- **Stable performance**: Consistent search latency and accuracy during continuous updates\n\n**Memory Requirements:**\n- HNSW centroid index: ~40 bytes per partition (for 1B vectors with 100K partitions = ~4GB)\n- Block mapping metadata: ~40 bytes per partition (~4GB for billion-scale)\n- Version tracking: 1 byte per vector for reassignment tracking (~1GB for billion-scale)\n- Total: ~10-20GB for billion-scale datasets\n\n**When to Use:**\n- Collections with frequent updates (>1% daily update rate)\n- Billion-scale datasets where global rebuilds are too expensive\n- Use cases requiring stable tail latency during updates\n- Applications needing fresh data without performance degradation\n\n**Configuration Parameters:**\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `maxPostingSize` | integer | 10000 | Maximum vectors per partition before split triggers |\n| `minPostingSize` | integer | 1000 | Minimum partition size before merge considered |\n| `reassignRange` | integer | 64 | Number of nearby partitions to check for reassignment |\n| `vectorCacheMaxObjects` | integer | 1e12 | Maximum objects in memory cache |\n| `ef` | integer | -1 | Search list size for HNSW centroid index (uses dynamic ef by default) |\n| `efConstruction` | integer | 128 | Build parameter for HNSW centroid index |\n\n**Limitations:**\n- Centroid HNSW index resides in memory (though small compared to full HNSW)\n- Requires sufficient disk IOPS for optimal performance\n- Initial index building still requires balanced clustering\n\n---\n\n## Draft Client API\n\n### Python Client v4\n\n```python\nfrom weaviate.classes.config import Configure, VectorDistances\n\nclient.collections.create(\n    \"Articles\",\n    vector_config=Configure.Vectors.text2vec_openai(\n        name=\"default\",\n        # Configure SPFresh index\n        vector_index_config=Configure.VectorIndex.spfresh(\n            distance_metric=VectorDistances.COSINE,\n            # Partition management\n            max_posting_size=10000,\n            min_posting_size=1000,\n            reassign_range=64,\n            # Centroid HNSW configuration\n            ef=-1,  # Dynamic ef for centroid search\n            ef_construction=128,  # Build quality for centroid index\n            vector_cache_max_objects=1000000000000\n        )\n    )\n)\n```\n\n### Migration from HNSW with Updates\n\n```python\n# Before: HNSW requiring periodic rebuilds\nclient.collections.create(\n    \"Articles\",\n    vector_config=Configure.Vectors.text2vec_openai(\n        vector_index_config=Configure.VectorIndex.hnsw()\n    )\n)\n\n# After: SPFresh for continuous updates\nclient.collections.create(\n    \"Articles\",\n    vector_config=Configure.Vectors.text2vec_openai(\n        vector_index_config=Configure.VectorIndex.spfresh(\n            max_posting_size=10000,\n            reassign_range=64\n        )\n    )\n)\n```",
    "focus": "Emphasize practical implementation, configuration examples, and when to choose SPFresh over HNSW for production use cases"
}